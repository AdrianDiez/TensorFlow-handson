{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorflowML\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0\n",
      "3.5858502\n"
     ]
    }
   ],
   "source": [
    "GRAPH = tf.Graph()\n",
    "SESSION = tf.InteractiveSession(graph=GRAPH)\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# MSE\n",
    "# ----------------------------------------\n",
    "X = tf.constant([[2, 4], [6, 8]],\n",
    "                dtype=tf.float32)\n",
    "X_HAT = tf.constant([[1, 2], [3, 4]],\n",
    "                    dtype=tf.float32)\n",
    "MSE = tf.nn.l2_loss(X - X_HAT)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Cross entropy\n",
    "# ----------------------------------------\n",
    "Y = tf.constant([[1, 0, 0, 1], [0, 1, 1, 1]],\n",
    "                dtype=tf.float32)\n",
    "Y_HAT = tf.constant([[0, 1, 0, 1], [1, 1, 1, 1]],\n",
    "                    dtype=tf.float32)\n",
    "CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=Y_HAT, labels=Y\n",
    "))\n",
    "# ----------------------------------------\n",
    "# Eval session values\n",
    "# ----------------------------------------\n",
    "(MSE_EVAL,\n",
    "CE_EVAL) = SESSION.run([MSE, CE])\n",
    "\n",
    "print(MSE_EVAL)\n",
    "print(CE_EVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Stochastic Gradient Descent\n",
    "\n",
    "Now we'll go through a full example. We'll use *housing* data from california and we'll train a linear regression model in order to predict the price. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Fetch data\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to scale the data whenever using gradient related methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(housing.data)\n",
    "scaled_housing_data = scaler.transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), \n",
    "                                      scaled_housing_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant nodes to hold the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(scaled_housing_data_plus_bias, \n",
    "               dtype=tf.float32, \n",
    "                name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1),\n",
    "               dtype=tf.float32, \n",
    "                name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define $\\theta$ (as a variable since it's going to be updated every iteration). Notice the dimension is $n+1$ since we are adding the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = tf.Variable(tf.random_uniform([n + 1, 1],\n",
    "                                     -1.0, \n",
    "                                     1.0),\n",
    "                   name=\"theta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are performing linear regression, our predictions are as follows $\\hat{y}=X\\theta + b$ => $\\hat{y}=X\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.matmul(X, theta, name=\"predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the residuals as follows:  $y-\\hat{y}$ and our loss function: $mse=\\frac{1}{m}\\sum_{i=0}^m\\|y_i - \\hat{y}_i\\|^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = y_pred - y \n",
    "mse = tf.reduce_mean(tf.square(error), \n",
    "                     name=\"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our loss function is as follows $mse=\\frac{1}{m}\\sum_{i=0}^m\\|y_i - \\hat{y}_i\\|^2=\\frac{1}{m}\\sum_{i=0}^m\\|y_i - X_i'\\theta\\|^2$ it's clear that the gradients are given by the following formula $\\nabla_{\\theta}mse=\\frac{2}{m}\\sum_{i=0}^m X_i'\\|y_i - \\hat{y}_i\\|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = 2/m * tf.matmul(tf.transpose(X),\n",
    "                            error,\n",
    "                            name=\"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update our parameters: $\\theta_{t+1} = \\theta_{t} - \\eta\\nabla_{\\theta_t}mse$ where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_op = tf.assign(theta, \n",
    "                        theta - learning_rate * gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are ready to initialize our variables and start running our process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  MSE =  7.4170833\n",
      "Epoch  100  MSE =  0.7461083\n",
      "Epoch  200  MSE =  0.63706625\n",
      "Epoch  300  MSE =  0.6047505\n",
      "Epoch  400  MSE =  0.58231485\n",
      "Epoch  500  MSE =  0.5661528\n",
      "Epoch  600  MSE =  0.55449426\n",
      "Epoch  700  MSE =  0.54608625\n",
      "Epoch  800  MSE =  0.54002035\n",
      "Epoch  900  MSE =  0.53564537\n",
      "Best theta\n",
      "[[ 2.0685525 ]\n",
      " [ 0.77509713]\n",
      " [ 0.14420804]\n",
      " [-0.09578247]\n",
      " [ 0.13598455]\n",
      " [ 0.00546521]\n",
      " [-0.04031712]\n",
      " [-0.765513  ]\n",
      " [-0.7265943 ]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000 \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run(init)\n",
    "     \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch \", \n",
    "                  epoch, \n",
    "                  \" MSE = \", \n",
    "                  mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    final_theta = theta.eval()\n",
    "    \n",
    "print('Best theta')\n",
    "print(final_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradients of complex functions\n",
    "\n",
    "In our last example we were able to compute the gradient explicitly. But what happens when we have an arbitrary complex function $\\hat{y}=f(x)$? How could we compute the gradient of a function such as my_func? hint: do not try to do it manually!\n",
    "\n",
    "```\n",
    "def my_func(a, b):\n",
    "   z = 0\n",
    "   for i in range(100):\n",
    "       z = a * np.cos(z + i) + z * np.sin(b - i)\n",
    "   return z\n",
    "```\n",
    "\n",
    "Luckily, TensorFlow can perform automatic differentiation, hence give us the gradient of arbitrarily complex functions. For example, if we want to know the value of the gradient of my_func at $a=.2$ y $b = .3$ we'll perform the following. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(a, b):\n",
    "    z = 0\n",
    "    for i in range(100):\n",
    "        z = a * np.cos(z + i) + z * np.sin(b - i)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21253923284754916"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_func(.2, .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a, b, z and the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(0.2, name=\"a\")\n",
    "b = tf.Variable(0.3, name=\"b\")\n",
    "z = tf.constant(0.0, name=\"z0\")\n",
    "## my_func\n",
    "for i in range(100):\n",
    "    z = a * tf.cos(z + i) + z * tf.sin(b - i)\n",
    "\n",
    "grads = tf.gradients(z, [a, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize variables and execute our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.21253741\n",
      "[-1.1388494, 0.19671395]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(z.eval())\n",
    "    print(sess.run(grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our linear regression example, we can replace the exact formula for the gradient by: \n",
    "\n",
    "```gradients = tf.gradients(mse, [theta])[0]```\n",
    "\n",
    "Notice that the arguments are, a function and a list of variables according to which compute the gradient. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  MSE =  7.41384\n",
      "Epoch  100  MSE =  0.7347319\n",
      "Epoch  200  MSE =  0.60109985\n",
      "Epoch  300  MSE =  0.57916105\n",
      "Epoch  400  MSE =  0.5647832\n",
      "Epoch  500  MSE =  0.554306\n",
      "Epoch  600  MSE =  0.54661655\n",
      "Epoch  700  MSE =  0.54095954\n",
      "Epoch  800  MSE =  0.53678596\n",
      "Epoch  900  MSE =  0.53369814\n",
      "Best theta\n",
      "[[ 2.0685523e+00]\n",
      " [ 7.3350579e-01]\n",
      " [ 1.2789957e-01]\n",
      " [-3.3120632e-02]\n",
      " [ 9.1169663e-02]\n",
      " [ 7.2845651e-05]\n",
      " [-3.8050886e-02]\n",
      " [-9.2345887e-01]\n",
      " [-8.8046956e-01]]\n"
     ]
    }
   ],
   "source": [
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "\n",
    "n_epochs = 1000 \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run(init)\n",
    "     \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch \", \n",
    "                  epoch, \n",
    "                  \" MSE = \", \n",
    "                  mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    final_theta = theta.eval()\n",
    "\n",
    "print('Best theta')\n",
    "print(final_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different optimizers\n",
    "\n",
    "If we wan't to use an optimization method different from simple gradient descent we can replace the ```training_op``` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  MSE =  13.034775\n",
      "Epoch  100  MSE =  0.8876547\n",
      "Epoch  200  MSE =  0.64736056\n",
      "Epoch  300  MSE =  0.61111987\n",
      "Epoch  400  MSE =  0.5872408\n",
      "Epoch  500  MSE =  0.5699978\n",
      "Epoch  600  MSE =  0.5575122\n",
      "Epoch  700  MSE =  0.5484657\n",
      "Epoch  800  MSE =  0.54190624\n",
      "Epoch  900  MSE =  0.537146\n",
      "Best theta\n",
      "[[ 2.0685525 ]\n",
      " [ 0.8085636 ]\n",
      " [ 0.15110648]\n",
      " [-0.157846  ]\n",
      " [ 0.18679295]\n",
      " [ 0.00754053]\n",
      " [-0.0416143 ]\n",
      " [-0.6838008 ]\n",
      " [-0.6487208 ]]\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run(init)\n",
    "     \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch \", \n",
    "                  epoch, \n",
    "                  \" MSE = \", \n",
    "                  mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    final_theta = theta.eval()\n",
    "    \n",
    "print('Best theta')\n",
    "print(final_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch training\n",
    "\n",
    "Let's suppose that we want to train our model using data batches instead of the whole dataset. In this case, we should define X and y as placeholders instead of constants. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\" ) \n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the code remains the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100 \n",
    "n_batches = int(np.ceil(m / batch_size ))\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  \n",
    "    indices = np.random.randint(m, size=batch_size)  \n",
    "    X_batch = scaled_housing_data_plus_bias[indices] \n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] \n",
    "    return X_batch, y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we run our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "[[ 2.070016  ]\n",
      " [ 0.8204561 ]\n",
      " [ 0.1173173 ]\n",
      " [-0.22739051]\n",
      " [ 0.3113402 ]\n",
      " [ 0.00353193]\n",
      " [-0.01126994]\n",
      " [-0.91643935]\n",
      " [-0.8795008 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print('''Epoch: {epoch}'''.format(epoch=epoch))\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    final_theta = theta.eval()\n",
    "              \n",
    "print(final_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to save our models we call ```tf.train.Saver()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 2.7544272\n",
      "Epoch 100 MSE = 0.63222194\n",
      "Epoch 200 MSE = 0.5727796\n",
      "Epoch 300 MSE = 0.5585005\n",
      "Epoch 400 MSE = 0.54906934\n",
      "Epoch 500 MSE = 0.5422877\n",
      "Epoch 600 MSE = 0.5373788\n",
      "Epoch 700 MSE = 0.5338219\n",
      "Epoch 800 MSE = 0.5312427\n",
      "Epoch 900 MSE = 0.52937055\n",
      "[[ 2.06855249e+00]\n",
      " [ 7.74078071e-01]\n",
      " [ 1.31192386e-01]\n",
      " [-1.17845066e-01]\n",
      " [ 1.64778143e-01]\n",
      " [ 7.44078017e-04]\n",
      " [-3.91945094e-02]\n",
      " [-8.61356676e-01]\n",
      " [-8.23479772e-01]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000                                                                       \n",
    "learning_rate = 0.01                                                                  \n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")            \n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")            \n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")                                      \n",
    "error = y_pred - y                                                                    \n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                                    \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)            \n",
    "training_op = optimizer.minimize(mse)                                                 \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())                                \n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    final_theta = theta.eval()\n",
    "    print(final_theta)\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to load our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n",
      "[[ 2.06855249e+00]\n",
      " [ 7.74078071e-01]\n",
      " [ 1.31192386e-01]\n",
      " [-1.17845066e-01]\n",
      " [ 1.64778143e-01]\n",
      " [ 7.44078017e-04]\n",
      " [-3.91945094e-02]\n",
      " [-8.61356676e-01]\n",
      " [-8.23479772e-01]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    final_theta_restored = theta.eval() \n",
    "\n",
    "print(final_theta_restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
